---
title: "Statistical Learning Project - Player position classification"
author: "Horatiu Andrei Palaghiu, Giovanni Dal Mas, Daniele Arsieni"
date: 
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Statistical Learning Project - Player position classification

### 1.Abstract and motivation

FIFA is one of the most known videogame and the most famous sport title in the industry, in particular we considered FIFA 22 edition. Each player covers a specific position on the field; what we want to do is building some models to classify the position of the player, based on the values of its attributes. It's important to consider that some players may share some features with footballers playing in another position, and this may influence our task. For example, some attacking midfielders (CAM) have a good shot and pace, just like wingers (RW, LW). We will keep this into account and adjust our classification accordingly.

### 2. The dataset - Description & EDA

The original dataset has been extracted from <https://sofifa.com/> and contains 19239 players described by 110 different features.

### 2.1 DataFrame inspection and rough slicing

```{r}
#first we import libraries, some of wich we never use, but just in case
library(knitr) 
library(ggplot2)    #graphs
library(reshape2)   #dataframe reshaping
library(viridis)    #colors
library(stringr)    #working with strings
library(FactoMineR) #factor analysis (PCA)
library(factoextra) #factor analysis (PCA)
library("stringr")

library(dplyr)
library(ggplot2)
library(MASS)
library(class)
library(gmodels)

library(tidyverse)

library(corrplot)
library(gridExtra)
library(reshape)
library(corrplot)
library(caret)
library(randomForest)
library(cvms)

library(leaps)
library(glmnet)
```

We set the seed for reproducible experiments

```{r}
set.seed(123)
```

First we load the dataset, and check the dimension.

```{r}
players_full <- read.csv("C:\\Users\\gioda\\Downloads\\archive (1)\\players_22.csv") #full dataframe
dim(players_full) #full dataset
```

We have more or less 20k players with 110 attributes. Below we look at how those attributes are named.

```{r}
colnames(players_full)
```

To get a better general idea, we also want to look at the type of data they provide

```{r}
head(players_full, 10)
```



We perform a rough removal of all the features that will obviously not be relevant to our classification, or some of the ones that are a obvious linear composition of other features. Moreover, since in the lower leagues the split between the attacking and defensive players is more vague, and the given ratings could be less trustworthy, the training will only be performed on the league 1 players. Then, we check the dimensions again.

```{r}

players_full <- players_full[players_full$league_level == 1,]

players_22 <- subset(players_full, select = c("short_name","player_positions","age","value_eur", "potential","height_cm","weight_kg","pace","shooting","passing","preferred_foot","weak_foot","dribbling","defending","physic","attacking_crossing","attacking_finishing","attacking_heading_accuracy","attacking_short_passing","attacking_volleys","skill_dribbling","skill_curve","skill_fk_accuracy","skill_long_passing","skill_ball_control","movement_acceleration","movement_sprint_speed","movement_agility","movement_reactions","movement_balance","power_shot_power","power_jumping","power_stamina","power_strength","power_long_shots","mentality_aggression","mentality_interceptions","mentality_positioning","mentality_vision","mentality_penalties","mentality_composure","defending_marking_awareness","defending_standing_tackle","defending_sliding_tackle"))


dim(players_22)

```

Apparently we kept only 44 features. Good enough. We will remove more later by performing feature selection so stay tuned.

```{r}
head(players_22, n=5)
```

We have a short look at the numerical summary of all the features we selected. On a first glance they look like they need some normalization. But before that, we would love to make some visual presentations.

```{r}
summary(players_22)
```



**2.2 Managing empty entries and goalkeepers**

We look at how many NAs we have on each attribute, in order to decide if we prefer removing them or filling them. In addiction, we came to the conclusion that goalkeepers

```{r}
players_22<-subset(players_22, player_positions!="GK")
which(apply(X = players_22, MARGIN = 2, FUN = anyNA) == TRUE) # check for NA
```

We decide that we have a statistically dispensable number of NAs so we remove them.

```{r}
players_22 <- na.omit(players_22) # delete NA
dim(players_22)
```

We still have a good chunk of the dataset left.

**2.3 Labelling**

Some players play in multiple positions, but we only want to identify their main one, so we only keep that one. Moreover, we turn the binary "preferred_foot" feature into a numerical type.

```{r}

#Keep only the main preferred position
players_22$player_positions<- word(players_22$player_positions, 1, sep = fixed(","))
unique(players_22$player_positions)

# Left foot is -1 and Right foot is 1. Basically one-hot encoding but we only have 2 categories so its easy
players_22$preferred_foot[players_22[,"preferred_foot"]== "Left"] <- as.numeric(-1)
players_22$preferred_foot[players_22[,"preferred_foot"]== "Right"] <- as.numeric(1)
players_22$preferred_foot <- as.numeric(players_22$preferred_foot)
# now we group them into the main 9 positions
```

Now, we take a look at the positions, and we plan to group them depending on the area of the field that they play in.

![](images/field_positions.jpeg)

Goalkeeper excluded, there are 26 positions, namely:

1.  LWB = Left Wing Back
2.  LB = Left Back
3.  LCB = Left Center Back
4.  CB = Center Back
5.  RCB = Right Center Back
6.  RB = Right Back
7.  RWB = Right Wing Back
8.  LDM = Left Defensive Midfield
9.  CDM = Center Defensive Midfield
10. RDM = Right Defensive Midfield
11. RCM = Right Center Midfield
12. CM = Center Midfield
13. LCM = Left Center Midfield
14. RAM = Right Attacking Midfield
15. CAM = Center Attacking Midfield
16. LAM = Left Attacking Midfield
17. LM = Left Midfield
18. RM = Right Midfield
19. LW = Left Winger
20. RW = Right Winger
21. LF = Left Forward
22. CF = Center Forward
23. RF = Right Striker
24. LS = Left Striker
25. ST = Striker
26. RS = Right Striker

As mentioned above, since 26 labels positions are clearly too many, we cluster them into nine classes of positions based on area of action on the field.

***Note:*** This is probably the only part where we applied our "domain knowledge".

```{r}

#central back
players_22$player_positions[players_22[,"player_positions"]== "LCB"|players_22[,"player_positions"]== "CB"|players_22[,"player_positions"]== "RCB"] <- "CB"

#left back 
players_22$player_positions[players_22[,"player_positions"]== "LWB"|players_22[,"player_positions"]== "LB"]<-"LB"

#right back 
players_22$player_positions[players_22[,"player_positions"]== "RWB"|players_22[,"player_positions"]== "RB"]<-"RB"

#center defensive midfield
players_22$player_positions[players_22[,"player_positions"]== "LDM"|players_22[,"player_positions"]== "CDM"|players_22[,"player_positions"]== "RDM"] <- "CDM"

#center midfield
players_22$player_positions[players_22[,"player_positions"]== "LCM"|players_22[,"player_positions"]== "CM"|players_22[,"player_positions"]== "RCM"] <- "CM"

#center attacking midfield
players_22$player_positions[players_22[,"player_positions"]== "LAM"|players_22[,"player_positions"]== "CAM"|players_22[,"player_positions"]== "RAM"] <- "CAM"

#left winger
players_22$player_positions[players_22[,"player_positions"]== "LM"|players_22[,"player_positions"]== "LW"|players_22[,"player_positions"]== "LF"] <- "LW"

#right winger
players_22$player_positions[players_22[,"player_positions"]== "RM"|players_22[,"player_positions"]== "RW"|players_22[,"player_positions"]== "RF"] <- "RW"

#striker
players_22$player_positions[players_22[,"player_positions"]== "LS"|players_22[,"player_positions"]== "CF"|players_22[,"player_positions"]== "RS"] <- "ST"

unique(players_22$player_positions)
```

Time to normalize the numerical values, as promised. For that, we implement a simple re-scaling function, and we apply it on the whole dataframe.

```{r}
# normalization function
normalize <-function(x) { (x -min(x))/(max(x)-min(x))   }

# normalize 
players_norm <- as.data.frame(lapply(players_22[, c(3:44)], normalize))
head(players_norm,5)
```

###
```{r}
colnames(players_norm)
```


##Regularization

#Ridge regression and Lasso

In this part we explore two shrinkage methods, namely Ridge regression and Lasso, also known as penalized regression methods. Through these techniques we can fit a model wher all the predictors are contained, but some coefficient estimates are shrinked towards zero.

```{r}
attach(players_norm)
X <- model.matrix(value_eur ~.,players_norm)
y <- value_eur
```


#Ridge with normalisation
Ridge uses  quadratic shrinking

```{r}
#ridge regression

grid.ridge <-10^seq(-4,2,length=100)
ridge.mod<-glmnet(X,y,alpha = 0,lambda = grid.ridge)
plot(ridge.mod, xvar="lambda", label= TRUE)

```
The plot illustrates how much the coefficients are penalized for different values of  Î». Notice none of the coefficients are forced to be zero

Looking at the plot the features:

[3]-potential
[27]-movement reactions

seem to have the biggest impact on predicting players' market value.

```{r}
#coeff for lambda[50]
dim(coef(ridge.mod))
ridge.mod$lambda[50]
coef(ridge.mod)[,50]

```


```{r}
# evaluate the MSE on a test set
set.seed(123)

# select n/2 observations for training set

train <- sample(1:nrow(X), nrow(X)/2)
test <- (-train)
y.test <- y[test]

```

```{r}
# fit ridge regression on the training set
ridge.mod <- glmnet(X[train, ], y[train], alpha = 0,
                    lambda = grid.ridge, thresh = 1e-12)

```

We estimate the test MSE for one lambda value, e.g lambda = 9
```{r}
#We estimate the test MSE for one lambda value, e.g lambda = 9

ridge.pred <- predict(ridge.mod, s = 9, newx = X[test, ], type="response")
mean((ridge.pred - y.test)^2)
```
let's see the coefficients for lambda = 9

```{r}
predict(ridge.mod, s = 9, exact = TRUE, type = "coefficients",
        x = X[train, ], y = y[train])[1:20, ]

```

We use cross-validation to choose the value of lambda

```{r}
set.seed(123)
cv.out.ridge <- cv.glmnet(X[train, ], y[train], alpha = 0, nfold=10)
```

```{r}
# lambda values

cv.out.ridge$lambda[1:10]

```
```{r}
summary(cv.out.ridge$lambda)
```

```{r}
# The mean cross-validated error

cv.out.ridge$cvm[1:10]
```
The following plot shows the cross-validation curve (red dotted line) with upper and lower standard deviation curves along the lambda sequence (error bars). 

Two special values along the lambda sequence are indicated by the vertical dotted lines:

lambda.min is the value of lambda that gives minimum mean cross-validated error, while lambda.1se is the value of lambda that gives the most regularized model such that the cross-validated error is within one standard error of the minimum.

```{r}
plot(cv.out.ridge)
```
```{r}
# identify the best lambda value

i.bestlam <- which.min(cv.out.ridge$cvm)
i.bestlam 
```
```{r}
bestlam <- cv.out.ridge$lambda[i.bestlam]
bestlam
```
```{r}
cv.out.ridge$cvm[i.bestlam]
```

```{r}
# estimate the test MSE 
ridge.pred <- predict(ridge.mod, s = bestlam,
                      newx = X[test, ])
#MSE
mean((ridge.pred - y.test)^2)
```

```{r}
# fit the coefficient with lambda=bestlam on all the data

out <- glmnet(X, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam)


```
```{r}
# residual sum of squares
ridge.RSS <- sum((ridge.pred - y.test)^2)
ridge.RSS
```

#Lasso
Lasso uses absolute-value shrinking

```{r}

X <- model.matrix(value_eur ~.,players_norm)
y<- value_eur
grid.lasso<-10^seq(-0.5,-8,length=100)
lasso.mod<-glmnet(X,y,alpha = 1,lambda = grid.lasso)
plot(lasso.mod, xvar="lambda", label = TRUE)
```
In the Lasso plot we can notice that some coefficients are forced to be zero

Looking at the plot the features, it's clear once again the big influence of [3]-potential and
[27]-movement reactions on the estimvtion of the market value


```{r}
set.seed(123)

train <- sample(1:nrow(X),nrow(X)/2)
test<-(-train)
y.test<-y[test]
lasso.pred<-predict(lasso.mod,s=4,newx = X[test,],type = "response")
mean((lasso.pred-y.test)^2)


```
We use cross-validation to choose the value of lambda

```{r}

cv.out.lasso <- cv.glmnet(X[train, ], y[train], alpha = 1,lambda = grid.lasso, nfold=10)

summary(cv.out.lasso$lambda)
```

```{r}
# select n/2 observations for training set

set.seed(123)
train <- sample(1:nrow(X), nrow(X)/2)
test <- (-train)
y.test <- y[test]
```

```{r}
# apply lasso to the training set 
lasso.mod <- glmnet(X[train,], y[train], alpha=1, lambda=grid.lasso)
```

```{r}
# apply 10fold cross-validation to the training set

set.seed(123)
cv.out.lasso <- cv.glmnet(X[train,], y[train], alpha=1)
plot(cv.out.lasso)
```

```{r}

# estimate test MSE
bestlam <- cv.out.lasso$lambda.min
lasso.pred <- predict(lasso.mod, s=bestlam, newx=X[test,])
mean((lasso.pred-y.test)^2)
```

```{r}
# fit the model with best-lambda on all the data
lasso.coef <- predict(lasso.mod,type="coefficients",s=bestlam)[1:43,]
lasso.coef
```
We check which coefficients were forced to zero 
```{r}
lasso.coef[lasso.coef!=0]
length(lasso.coef[lasso.coef!=0])
which(lasso.coef==0)

```

```{r}
# residual sum of squares
lasso.RSS <- sum((lasso.pred - y.test)^2)
lasso.RSS

```
We use the variables selected by Lasso method (i.e. non-zero coefficients) and fit the resulting least squares model
```{r}

#train
lm.lasso <- lm(value_eur~.-shooting-passing- dribbling-defending-physic- attacking_heading_accuracy-power_jumping- mentality_positioning-defending_sliding_tackle, data=players_norm[train,])

```

```{r}
#test

lm.lasso.pred <- predict(lm.lasso, newdata=players_norm[test,])

mean((lm.lasso.pred-y.test)^2)
```

```{r}
# residual sum of squares
lm.lasso.RSS <- sum((lm.lasso.pred - y.test)^2)
lm.lasso.RSS
```


```{r}
# compare with best subset selection using BIC and AIC

# apply best subset selection on the training data
library(leaps)
regfit.full <- regsubsets(value_eur~., data=players_norm[train,])
reg.summary <- summary(regfit.full)
reg.summary

```

```{r}

# select the model with best BIC
which.min(reg.summary$bic)
```

```{r}
coef(regfit.full, 7)
```
```{r}
mod.bic <- lm(value_eur~potential+height_cm+pace+movement_agility+movement_reactions+movement_balance+mentality_vision, data = players_norm[train,])
```

```{r}
bic.pred <- predict(mod.bic, newdata=players_norm[test,])
mean((bic.pred-y.test)^2)

```

```{r}
# select the model with best AIC
which.min(reg.summary$cp)

```
```{r}
coef(regfit.full, 8)
```

```{r}
mod.aic <- lm(value_eur~potential+height_cm+pace+skill_dribbling+movement_agility+movement_reactions+movement_balance+mentality_vision, data = players_norm[train,])
```

```{r}
aic.pred <- predict(mod.aic, newdata=players_norm[test,])
mean((aic.pred-y.test)^2)
```
##Best Subset selection (to check)

```{r}
fit_all = regsubsets(value_eur ~ ., data = players_norm, nvmax = 19)
fit_all_sum = summary(fit_all)
names(fit_all_sum)
```

```{r}
fit_all_sum$bic
```

```{r}
par(mfrow = c(2, 2))
plot(fit_all_sum$rss, xlab = "Number of Variables", ylab = "RSS", type = "b")

plot(fit_all_sum$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "b")
best_adj_r2 = which.max(fit_all_sum$adjr2)
points(best_adj_r2, fit_all_sum$adjr2[best_adj_r2],
       col = "red",cex = 2, pch = 20)

plot(fit_all_sum$cp, xlab = "Number of Variables", ylab = "Cp", type = 'b')
best_cp = which.min(fit_all_sum$cp)
points(best_cp, fit_all_sum$cp[best_cp], 
       col = "red", cex = 2, pch = 20)

plot(fit_all_sum$bic, xlab = "Number of Variables", ylab = "BIC", type = 'b')
best_bic = which.min(fit_all_sum$bic)
points(best_bic, fit_all_sum$bic[best_bic], 
       col = "red", cex = 2, pch = 20)
```


**2.3 Correlation matrix and feature selection**

We create a correlation matrix. It is big and maybe a bit hard to read, but R gives us the visually appealing option to group plotted features into highly correlated clusters.

```{r}

cormatrix <- cor(players_norm)
corrplot(cor(players_norm), method = 'shade', sig.level = 0.10, type = 'lower', order = 'hclust', title = "Correlation plot before feature selection")
```

Now, in order to reduce the number of features, we take away the ones that provide the data with the highest overall correlation.

```{r}
highcorr <- findCorrelation(cormatrix, cutoff=0.8)
highcorr
col2<-colnames(players_norm)
col2
col2<-col2[-highcorr]
corrplot.mixed(cor(players_norm[highcorr]), lower = "number", upper="shade", tl.pos = 'lt')
```

Now we take a look if we eliminated some of the dark spots from our correlation matrix.

```{r}
corrplot(cor(players_norm[col2]), type = 'lower',method = 'shade', order = 'hclust', title = "Correlation plot after feature selection")
players_model <- subset(players_norm)
#we can add the positions back
players_model$player_positions <- c(players_22$player_positions)
```

We did. Looks much better and ready for further investigation.

**2.4 Individual feature investigation**

We want to look at the individual distributions of each of the features left. We fit violin plots, and put boxplots on top of them.

```{r}
#here we do the cool violin plots to check distributions
par(mfrow=c(4,2))
ggplot(data = melt(players_norm[,1:5]), aes(y = variable, x = value, fill = variable, alpha = 0.7)) + geom_boxplot() + geom_violin() + scale_fill_manual(values = viridis(5)) + guides(fill = "none")
```

a

```{r}
ggplot(data = melt(players_norm[,6:10]), aes(y = variable, x = value, fill = variable, alpha = 0.7)) + geom_boxplot() + geom_violin() + scale_fill_manual(values = viridis(5)) + guides(fill = "none")
```

Weak foot is a discrete RV with values in 1-5. Preferred foot is +/-1, as discussed above. Still, as in real life, a significantly larger proportion of right-footed people.

```{r}

ggplot(data = melt(players_norm[,11:15]), aes(y = variable, x = value, fill = variable, alpha = 0.7)) + geom_boxplot() + geom_violin() + scale_fill_manual(values = viridis(5)) + guides(fill = "none")
```

aaa

```{r}

ggplot(data = melt(players_norm[,16:20]), aes(y = variable, x = value, fill = variable, alpha = 0.7)) + geom_boxplot() + geom_violin() + scale_fill_manual(values = viridis(5)) + guides(fill = "none")
```

aaa

```{r}

ggplot(data = melt(players_norm[,21:25]), aes(y = variable, x = value, fill = variable, alpha = 0.7)) + geom_boxplot() + geom_violin() + scale_fill_manual(values = viridis(5)) + guides(fill = "none")
```

aaa

```{r}

ggplot(data = melt(players_norm[,26:30]), aes(y = variable, x = value, fill = variable, alpha = 0.7)) + geom_boxplot() + geom_violin() + scale_fill_manual(values = viridis(5)) + guides(fill = "none")
```

aaa

```{r}

ggplot(data = melt(players_norm[,31:35]), aes(y = variable, x = value, fill = variable, alpha = 0.7)) + geom_boxplot() + geom_violin() + scale_fill_manual(values = viridis(5)) + guides(fill = "none")
```

aaa

```{r}

ggplot(data = melt(players_norm[,36:40]), aes(y = variable, x = value, fill = variable, alpha = 0.7)) + geom_boxplot() + geom_violin() + scale_fill_manual(values = viridis(5)) + guides(fill = "none")
```

aaa

```{r}

```

**2.5 Principal Component Analysis**

```{r}
players.pca<-prcomp(players_norm,center=TRUE, scale.=TRUE)
summary(players.pca)
```

We obtain 40 components. We want to visualise them.

```{r}
fviz_eig(players.pca, addlabels = TRUE)
```

The first 5 components account for 77.7% of the explained variance, while the first 2 for 58.3% of it. Now we want to see how our features project into the main 2D factor plane.

```{r}
fviz_pca_var(players.pca, labelsize = 2, alpha.var = 1.0, title = "Factor Plane for the FIFA 22 Data")
```

### 3. Modelling - Multiclass classification

Now its finally time to dive into the actual modelling process. We experiment and compare different classification algorithms.

**3.1 Train-validation split**

Classical split for training and testing models. We keep the classical 70%-30% approach.

```{r}
## 70% of the sample size
smp_size <- floor(0.7 * nrow(players_model))

train_ind <- sample(seq_len(nrow(players_model)), size = smp_size)

train <- players_model[train_ind, ]
test <- players_model[-train_ind, ]

print('Train set size:')
print(dim(test))
print('Validation set size:')
print(dim(train))
```

We factorise the labes, so we can use them in our models.

```{r}
#factorise labels
train_y <- as.factor(train[,41])
test_y <- as.factor(test[,41])
```

Just to take a sneak peek, this is how the validation labels are roughly distributed on the factor plane.We notice that the factor plane separates some types of labels quite good, some not.

```{r}
test.pca<-prcomp(test,center=TRUE, scale.=TRUE)
fviz_pca_biplot(test.pca,
                label = "all",
                col.ind = test_y,
                legend.title = "Players",
                title = "Classification of players")
```

**3.2 Useful functions**

Before we train any model, we want to create a function that computes accuracy, and one that selects the missclassified data so we can visualize it later on the factor plane.

```{r}
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
missclassified <- function(pred, label){
  l<- pred
  l[c(pred)==c(label)]<- none
  return (as.factor(l))
}
```

**3.3 Knn**

```{r}
##run knn function
class <- factor(c(train_y))

train <- train[1:(length(train)-1)]
test <- test[1:(length(test)-1)]

accuracy_vect <- c()
ks<- c()

for(k1 in seq(5,100,5)) {
    test_pred <-knn(train = train, test = test, cl = class, k = k1)
    accuracy_vect <- append(accuracy_vect,accuracy(table(test_y,test_pred)))
    ks <- append(ks, k1)
}

plot(ks, accuracy_vect, type = "p", col="blue", xlab="K's", ylab="accuracys", main="Accuracy vs K value plot")
```

We get the best k and its accuracy.

```{r}
print('The best K in our case is:')
print(ks[which.max(accuracy_vect)])
print('And it gives us an accuracy of:' )
print(accuracy_vect[which.max(accuracy_vect)])
```

```{r}
test_pred <-knn(train = train, test = test, cl = class, k = 40)
df_pred=data.frame(test_y,test_pred)

```

We generate a confusion matrix to check misslabeled data

```{r}
#Evaluate the model performance
CrossTable(x=test_y, y=test_pred,prop.chisq = FALSE)
```

```{r}
#creating confusion matrix
conf_mat <- confusion_matrix(targets = test_y,
                             predictions = test_pred)
```

Now we visualise it on the factor plane

```{r}


fviz_pca_biplot(test.pca,
                label = "all",
                col.ind = missclassified(test_pred,test_y),
                legend.title = "Players",
                title = "Classification of labeled/misslabeled players for KNN")

```

**3.4 Random Forrest**

The hyperparameter we experiment with is the number of randomly sampled variables. Changing the number of trees does not do much, and from previous experimentation we realized that around 500 is the optimum value.

```{r}
a=c()
i=5
for (i in 5:10) {
  model_RF <- randomForest(train_y ~ ., data = train, ntree = 500, mtry = i, importance = TRUE)
  prediction_RF <- predict(model_RF, test, type = "class")
  a[i-4] = mean(prediction_RF == test_y)
}
plot(5:10,a)
```

```{r}
a
```

a = 8 is the best one.

We plot missclassified labels again on the factor plane.

```{r}
model_RF <- randomForest(train_y ~ ., data = train, ntree = 500, mtry = 8, importance = TRUE)
prediction_RF <- predict(model_RF, test, type = "class")
fviz_pca_biplot(test.pca,
                label = "all",
                col.ind = missclassified(prediction_RF,test_y),
                legend.title = "Players",
                title = "Classification of labeled/misslabeled players for RF")
```

## 3.5 SVM

```{r}
svm1 <- svm(formula= train_y~., data=train, 
          type="C-classification", kernal="radial", 
          gamma=0.1, cost=10)
```

We produce a summary of the model.

```{r}
prediction_svm <- predict(svm1,test, type = "class")
accuracy(table(test_y, prediction_svm))
```

```{r}
summary(svm1)

```

We plot misslabeled data

```{r}
fviz_pca_biplot(test.pca,
                label = "all",
                col.ind = missclassified(prediction_svm,test_y),
                legend.title = "Players",
                title = "Classification of labeled/misslabeled players for SVM")
```

**3.6 Label Grouping**

The accuracies obtained are decent but not great, and the confusion matrix clearly explains why. Positions like CB, ST, LB, RB get classified really well. On the opposite side, the most commonly misclassified position are CAM with CM, and RW with LW and viceversa.

The first misclassification is explainable with basic attributes of the role. Centrer Attacking Midfielder shares a lot of attacking characteristics with the Winger such as shooting and pace but also many with CM, like passing.

The second one is a bit more tricky to detect. For Left Back and Right Back the preferred foot plays a big role, since it's hard to find a righty who plays on the left and viceversa, because they cross and tackle mostly with their dominant foot. For RW and LW the distinction is less definable based on the preferred foot. On one hand, a lot of righty players like to play as Left Winger so they can converge to the center to shoot with their strong foot. Same is true for lefty on RW. On the other hand, many Wingers like to cross more, so they tend to do it with their preferred foot (LW with left and RW with right). So for the model of course it's really not an easy job to detect these differences that pertain to the single player style of play; and this problem explains the drop in accuracy for these positions. In order to improve the accuracy of our classifiers, we group RW and LW together in a new position 'W = Winger' and the CAM with CM.

```{r}
test_y2 <- test_y
levels(test_y2)[levels(test_y2) == "RW"| levels(test_y2) == "LW"] <- "W"
levels(test_y2)[levels(test_y2) == "CAM"| levels(test_y2) == "CM"] <- "CM"
unique(test_y2)

train_y2 <- train_y
levels(train_y2)[levels(train_y2) == "RW"| levels(train_y2) == "LW"] <- "W"
levels(train_y2)[levels(train_y2) == "CAM"| levels(train_y2) == "CM"] <- "CM"

```

**3.6.1 Knn**

```{r}
 prediction_knn2 <-knn(train = train, test = test, cl = train_y2, k = 20)
 CrossTable(x=test_y2, y=prediction_knn2,prop.chisq = FALSE)
```

```{r}
  accuracy(table(prediction_knn2, test_y2))
```

```{r}
fviz_pca_biplot(test.pca,
                label = "all",
                col.ind = missclassified(prediction_knn2, test_y2),
                legend.title = "Players",
                title = "Classification of labeled/misslabeled players for KNN2")
```

**3.6.2 Random Forest**

```{r}
 model_RF2 <- randomForest(train_y2 ~ ., data = train, ntree = 500, mtry = 8, importance = TRUE)
prediction_RF2 <- predict(model_RF2, test, type = "class")
accuracy(table(prediction_RF2, test_y2))

```

```{r}
fviz_pca_biplot(test.pca,
                label = "all",
                col.ind = missclassified(prediction_RF2,test_y2),
                legend.title = "Players",
                title = "Classification of labeled/misslabeled players for RF2")
```

**3.6.3 SVM**

```{r}
svm2 <- svm(formula= train_y2~., data=train, 
          type="C-classification", kernal="radial", 
          gamma=0.1, cost=10)
prediction_svm2 <- predict(svm2, test, type = "class")
accuracy(table(test_y2, prediction_svm2))

```

```{r}
fviz_pca_biplot(test.pca,
                label = "all",
                col.ind = missclassified(prediction_RF2,test_y2),
                legend.title = "Players",
                title = "Classification of labeled/misslabeled players for RF2")
```

**4. Conclusion and further remarks**
