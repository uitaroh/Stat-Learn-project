---
title: "Statistical Learning Project - Player position classification and market value estimation"
author: "Horatiu Andrei Palaghiu, Giovanni Dal Mas, Daniele Arsieni"
date: 
output:
  pdf_document: default
  latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Statistical Learning Project - Player position classification and market value estimation

### 1.Abstract and motivation

FIFA is one of the most known videogames and the most famous sport title in the industry, in particular we considered FIFA 22 edition.

Each player covers a specific position on the field; what we want to do in the first part of the project is building some models to classify the position of the player, based on the values of its attributes. It's important to consider that some players may share some features with footballers playing in another position, and this may influence our task. For example, some attacking midfielders (CAM) have a good shot and pace, just like wingers (RW, LW). We will keep this into account and adjust our classification accordingly.

In the second part of our project we will perform a regression task, building some models to evaluate the market price of a football player, using Ridge and Lasso regression.

### 2. The dataset - Description &EDA

The original dataset has been extracted from <https://sofifa.com/> and contains 19239 players described by 110 different features.

### 2.1 DataFrame inspection and rough slicing

```{r,echo=FALSE}
#first we import libraries, some of wich we never use, but just in case
library(knitr) 
library(ggplot2)    #graphs
library(reshape2)   #dataframe reshaping
library(viridis)    #colors
library(stringr)    #working with strings
library(FactoMineR) #factor analysis (PCA)
library(factoextra) #factor analysis (PCA)
library("stringr")

library(dplyr)
library(ggplot2)
library(MASS)
library(class)
library(gmodels)

library(tidyverse)
library(stringr)
library(corrplot)
library(gridExtra)
library(reshape)
library(corrplot)
library(caret)
library(randomForest)
library(cvms) 
library(e1071)
library(leaps)
library(data.table)
library(leaps)
library(glmnet)
```

We set the seed for reproducible experiments

```{r}
set.seed(123)
```

First we load the dataset, and check the dimension.

```{r}
players_full <- read.csv("E:/horatiu/Documents/players_22.csv") #full dataframe
dim(players_full) #full dataset
```

We have more or less 20k players with 110 attributes. Below we look at how those attributes are named.

```{r}
colnames(players_full)
```

To get a better general idea, we also want to look at the type of data they provide

```{r}
head(players_full, 10)
```

We perform a rough removal of all the features that will obviously not be relevant to our classification, or some of the ones that are a obvious linear composition of other features. Moreover, our training will be performed on the league 1 players. Then, we check the dimensions again.

```{r}

players_full <- players_full[players_full$league_level == 1,]

players_22 <- subset(players_full, select = c("short_name","player_positions","age","potential", "international_reputation", "value_eur", "height_cm","weight_kg","pace","shooting","passing","preferred_foot","weak_foot","dribbling","defending","physic","attacking_crossing","attacking_finishing","attacking_heading_accuracy","attacking_short_passing","attacking_volleys","skill_dribbling","skill_curve","skill_fk_accuracy","skill_long_passing","skill_ball_control","movement_acceleration","movement_sprint_speed","movement_agility","movement_reactions","movement_balance","power_shot_power","power_jumping","power_stamina","power_strength","power_long_shots","mentality_aggression","mentality_interceptions","mentality_positioning","mentality_vision","mentality_penalties","mentality_composure","defending_marking_awareness","defending_standing_tackle","defending_sliding_tackle"))



dim(players_22)

```

Apparently we kept only 45 features. Good enough. We will remove more later by performing feature selection so stay tuned.

We have a short look a numerical summary of all the features we selected. On a first glance they look like they need some normalization. But before that, we would love to make some visual presentations.

```{r}
summary(players_22)
```

**2.2 Managing empty entries**

We look at how many NAs we have on each attribute, in order to decide if we prefer removing them or filling them.

```{r}
set.seed(123)
which(apply(X = players_22, MARGIN = 2, FUN = anyNA) == TRUE) # check for NA
```

We decide that we have a statistically dispensable number of NAs so we remove them.

```{r}
set.seed(123)
players_22 <- na.omit(players_22) # delete NA
dim(players_22)
```

We still have a good chunk of the dataset left. Since goalkeepers have special stats, we also would like to take them out. First, we check how many we have.

```{r}
set.seed(123)
goalkeepers <- str_detect(players_22$player_positions, "GK")
sum(goalkeepers)
```

Thus, while they are indisposable on the field, we could not say the same about their data, as it would reduce the accuracy of the classification of the other main positions.

**2.3 Subset creation**

We create two subsets: one with value_eur that we will use for the Regression on the market price and the subset players_22 that will be used for th position classification.

```{r}
#subset WITH market value (for later)
set.seed(123)
players_22b <- as.data.frame(copy(players_22))

players_m <-subset(players_22b, player_positions!="GK")

#subset WITHOUT market value
players_22 <-subset(players_22, player_positions!="GK", select = -value_eur)
```

**2.4 Labelling**

Some players play in multiple positions, but we only want to identify their main one, so we only keep that one. Moreover, we turn the binary "preferred_foot" feature into a numerical type.

```{r}
set.seed(123)

#Keep only the main preferred position
players_22$player_positions<- word(players_22$player_positions, 1, sep = fixed(","))
unique(players_22$player_positions)

# Left foot is -1 and Right foot is 1. Basically one-hot encoding but we only have 2 categories so its easy
set.seed(123)
players_22$preferred_foot[players_22[,"preferred_foot"]== "Left"] <- as.numeric(-1)
players_22$preferred_foot[players_22[,"preferred_foot"]== "Right"] <- as.numeric(1)
players_22$preferred_foot <- as.numeric(players_22$preferred_foot)
# now we group them into the main 9 positions
```

Now, we take a look at the positions, and we plan to group them depending on the area of the field that they play in.

![](field_positions.jpeg)

Goalkeeper excluded, there are 26 positions, namely:

1.  LWB = Left Wing Back
2.  LB = Left Back
3.  LCB = Left Center Back
4.  CB = Center Back
5.  RCB = Right Center Back
6.  RB = Right Back
7.  RWB = Right Wing Back
8.  LDM = Left Defensive Midfield
9.  CDM = Center Defensive Midfield
10. RDM = Right Defensive Midfield
11. RCM = Right Center Midfield
12. CM = Center Midfield
13. LCM = Left Center Midfield
14. RAM = Right Attacking Midfield
15. CAM = Center Attacking Midfield
16. LAM = Left Attacking Midfield
17. LM = Left Midfield
18. RM = Right Midfield
19. LW = Left Winger
20. RW = Right Winger
21. LF = Left Forward
22. CF = Center Forward
23. RF = Right Striker
24. LS = Left Striker
25. ST = Striker
26. RS = Right Striker

As mentioned above, since 26 labels positions are clearly too many, we cluster them into nine classes of positions based on area of action on the field.

***Note:*** This is probably the only part where we applied our "domain knowledge".

```{r}
set.seed(123)

#central back
players_22$player_positions[players_22[,"player_positions"]== "LCB"|players_22[,"player_positions"]== "CB"|players_22[,"player_positions"]== "RCB"] <- "CB"

#left back
players_22$player_positions[players_22[,"player_positions"]== "LWB"|players_22[,"player_positions"]== "LB"]<-"LB"

#right back
players_22$player_positions[players_22[,"player_positions"]== "RWB"|players_22[,"player_positions"]== "RB"]<-"RB"

#central deffensive midfielder
players_22$player_positions[players_22[,"player_positions"]== "LDM"|players_22[,"player_positions"]== "CDM"|players_22[,"player_positions"]== "RDM"] <- "CDM"

#central midfielder
players_22$player_positions[players_22[,"player_positions"]== "LCM"|players_22[,"player_positions"]== "CM"|players_22[,"player_positions"]== "RCM"] <- "CM"

#central attacking midfielder
players_22$player_positions[players_22[,"player_positions"]== "LAM"|players_22[,"player_positions"]== "CAM"|players_22[,"player_positions"]== "RAM"] <- "CAM"

#left winger
players_22$player_positions[players_22[,"player_positions"]== "LM"|players_22[,"player_positions"]== "LW"|players_22[,"player_positions"]== "LF"] <- "LW"

#right winger
players_22$player_positions[players_22[,"player_positions"]== "RM"|players_22[,"player_positions"]== "RW"|players_22[,"player_positions"]== "RF"] <- "RW"

#striker
players_22$player_positions[players_22[,"player_positions"]== "LS"|players_22[,"player_positions"]== "CF"|players_22[,"player_positions"]== "RS"] <- "ST"

```

Lets take a look at the distribution of our labels

```{r}
set.seed(123)
cat<- table(factor(players_22$player_positions))
pie(cat,
    col = hcl.colors(length(cat), "BluYl"))
```

Time to normalize the numerical values, as promised. For that, we implement a simple re-scaling function, and we apply it on the whole dataframe.

```{r}
set.seed(123)

# normalization function
normalize <-function(x) { (x -min(x))/(max(x)-min(x))   }

# normalize 
players_norm <- as.data.frame(lapply(players_22[, c(3:42)], normalize))
head(players_norm,5)
```

**2.5 Correlation matrix and feature selection**

We create a correlation matrix. It is big and maybe a bit hard to read, but R gives us the visually appealing option to group plotted features into highly correlated clusters.

```{r}
set.seed(123)
cormatrix <- cor(players_norm)
corrplot(cor(players_norm), method = 'shade', sig.level = 0.10, type = 'lower', order = 'hclust', title = "Correlation plot before feature selection")
```

Now, in order to reduce the number of features, we take away the ones that provide the data with the highest overall correlation.

```{r}
set.seed(123)
highcorr <- findCorrelation(cormatrix, cutoff=0.8)
highcorr
col2<-colnames(players_norm)
col2
col2<-col2[-highcorr]
corrplot.mixed(cor(players_norm[highcorr]), lower = "number", upper="shade", tl.pos = 'lt')
```

We take a look if we eliminated some of the dark spots from our correlation matrix.

```{r}
set.seed(123)

corrplot(cor(players_norm[col2]), type = 'lower',method = 'shade', order = 'hclust', title = "Correlation plot after feature selection")
players_f.selected <- players_norm[col2]  #features left
players_model <- subset(players_f.selected)

#we can add the positions back
players_model$player_positions <- c(players_22$player_positions)
```

```{r}
set.seed(123)
colnames(players_model)

```

We did. Looks much better and ready for further investigation.

**2.6 Best Subset selection**

```{r}
which(apply(X = players_model, MARGIN = 2, FUN = anyNA) == TRUE) # check for NA

fit_all <- regsubsets(as.factor(player_positions) ~ .,data=players_model)
summary(fit_all)
```

**2.6 Individual feature investigation**

We want to look at the individual distributions of each of the features left. We fit violin plots, and put boxplots on top of them.

```{r}
set.seed(123)
#here we do the cool violin plots to check distributions
par(mfrow=c(4,2))
ggplot(data = melt(players_f.selected[,1:4]), aes(y = variable, x = value, fill = variable, alpha = 0.7)) + geom_boxplot() + geom_violin() + scale_fill_manual(values = viridis(5)) + guides(fill = "none")
```

```{r}
set.seed(123)
ggplot(data = melt(players_f.selected[,5:8]), aes(y = variable, x = value, fill = variable, alpha = 0.7)) + geom_boxplot() + geom_violin() + scale_fill_manual(values = viridis(5)) + guides(fill = "none")
```

Weak foot is a discrete RV with values in 1-5. Preferred foot is +/-1, as discussed above. Still, as in real life, a significantly larger proportion of right-footed people.

```{r}
set.seed(123)
ggplot(data = melt(players_f.selected[,9:12]), aes(y = variable, x = value, fill = variable, alpha = 0.7)) + geom_boxplot() + geom_violin() + scale_fill_manual(values = viridis(5)) + guides(fill = "none")
```

```{r}
set.seed(123)
ggplot(data = melt(players_f.selected[,13:16]), aes(y = variable, x = value, fill = variable, alpha = 0.7)) + geom_boxplot() + geom_violin() + scale_fill_manual(values = viridis(5)) + guides(fill = "none")
```

```{r}
set.seed(123)
ggplot(data = melt(players_f.selected[,17:20]), aes(y = variable, x = value, fill = variable, alpha = 0.7)) + geom_boxplot() + geom_violin() + scale_fill_manual(values = viridis(5)) + guides(fill = "none")
```

```{r}
set.seed(123)
ggplot(data = melt(players_f.selected[,21:24]), aes(y = variable, x = value, fill = variable, alpha = 0.7)) + geom_boxplot() + geom_violin() + scale_fill_manual(values = viridis(5)) + guides(fill = "none")
```

### 3. Modelling - Multiclass classification

Now its finally time to dive into the actual modelling process. We experiment and compare different classification algorithms.

**3.1 Train-validation split**

Classical split for training and testing models. We keep the classical 70%-30% approach.

```{r}
set.seed(123)
## 70% of the sample size
smp_size <- floor(0.7 * nrow(players_model))

train_ind <- sample(seq_len(nrow(players_model)), size = smp_size)

train <- players_model[train_ind, ]
test <- players_model[-train_ind, ]

print('Train set size:')
print(dim(train))
print('Validation set size:')
print(dim(test))
```

We factorise the labels, so we can use them in our models.

```{r}
set.seed(123)
#factorise labels
train_y <- as.factor(train[,25])
test_y <- as.factor(test[,25])
#remove labels from sets
train <- train[1:(length(train)-1)]
test <- test[1:(length(test)-1)]
head(test)
```

Just to take a sneak peek, this is how the validation labels are roughly distributed on the factor plane.We notice that the factor plane sepparates some types of labels quite good, some not.

```{r}
set.seed(123)
test.pca<-prcomp(test,center=TRUE, scale.=TRUE)
fviz_pca_biplot(test.pca,
                label = "all",
                col.ind = test_y,
                legend.title = "Players",
                title = "Classification of players")
```

**3.2 Useful functions**

Before we train any model, we want to create a function that computes accuracy, and one that selects the missclassified data so we can visualize it later on the factor plane.

```{r}
set.seed(123)
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}

missclassified <- function(pred, label){
  l<- pred
  l[c(pred)==c(label)]<- 0
  return (as.factor(l))
}

#cross validation
ctrl <-  trainControl(method = "repeatedcv", number = 100, repeats = 10)
```

**3.3 Knn**

```{r}
set.seed(123)
##run knn function
class <- factor(c(train_y))

train <- train[1:(length(train)-1)]
test <- test[1:(length(test)-1)]

accuracy_vect <- c()
ks<- c()

for(k1 in seq(1,100)) {
    test_pred <-knn(train = train, test = test, cl = class, k = k1)
    accuracy_vect <- append(accuracy_vect,accuracy(table(test_y,test_pred)))
    ks <- append(ks, k1)
}

plot(ks, accuracy_vect, type = "p", col="blue", xlab="K's", ylab="accuracy", main="Accuracy vs K value plot")
abline(h = max(accuracy_vect), col = "darkorange", lty = 3)
```

We get the best k and its accuracy, represented by the orange line.

```{r}
set.seed(123)
print('The best K in our case is:')
print(ks[which.max(accuracy_vect)])
print('And it gives us an accuracy of:' )
print(accuracy_vect[which.max(accuracy_vect)])
```

```{r}
set.seed(123)
test_pred <-knn(train = train, test = test, cl = class, k = 51)
df_pred=data.frame(test_y,test_pred)

```

We generate a confusion matrix to check misslabeled data

```{r}
set.seed(123)
#Evaluate the model performance
CrossTable(x=test_y, y=test_pred,prop.chisq = FALSE)
```

**3.4 Logistic regression**

```{r}
set.seed(123)
modellr <- nnet::multinom( train_y~., data = train,trControl=crtl)
```

```{r}
set.seed(123)
prediction_lr <- predict(modellr, test, type = "class")
accuracy(table(test_y, prediction_lr))
```

Now we create a confusion matrix

```{r}
set.seed(123)
conf_mat <- confusionMatrix(data=prediction_lr, reference = test_y)
conf_mat
```

**3.5 Linear Discriminant Analysis**

```{r}
set.seed(123)
model_lda <- lda(train_y~., data = train)
prediction_lda <-  predict(model_lda, test, type = "class")
summary(model_lda)
```

```{r}

set.seed(123)
accuracy(table(test_y, prediction_lda$class))
```

Now we plot the obtained model

```{r}

set.seed(123)
plot(model_lda)
```

```{r}
set.seed(123)
confusionMatrix(data=prediction_lda$class, reference = test_y)
```

**3.6 Quadratic Discriminant Analysis**

```{r}
set.seed(123)
model_qda <- qda(train_y~., data = train)
prediction_qda <-  predict(model_qda, test, type = "class")
summary(model_qda)
```

```{r}
set.seed(123)
accuracy(table(test_y, prediction_qda$class))

```

```{r}
set.seed(123)
confusionMatrix(data=prediction_lda$class, reference = test_y)
```

**3.7 Label Grouping**

The accuracies obtained are decent but not great, and the confusion matrix clearly explains why. Positions like CB, ST, LB, RB get classified really well. On the opposite side, the most commonly misclassified position are CAM with CM, and RW with LW and viceversa.

The first misclassification is explainable with basic attributes of the role. Centrer Attacking Midfielder shares a lot of attacking characteristics with the Winger such as shooting and pace but also many with CM, like passing.

The second one is a bit more tricky to detect. For Left Back and Right Back the preferred foot plays a big role, since it's hard to find a righty who plays on the left and viceversa, because they cross and tackle mostly with their dominant foot. For RW and LW the distinction is less definable based on the preferred foot. On one hand, a lot of righty players like to play as Left Winger so they can converge to the center to shoot with their strong foot. Same is true for lefty on RW. On the other hand, many Wingers like to cross more, so they tend to do it with their preferred foot (LW with left and RW with right). So for the model of course it's really not an easy job to detect these differences that pertain to the single player style of play; and this problem explains the drop in accuracy for these positions. In order to improve the accuracy of our classifiers, we group RW and LW together in a new position 'W = Winger' and the CAM with CM.

```{r}
set.seed(123)
test_y2 <- test_y
levels(test_y2)[levels(test_y2) == "RW"| levels(test_y2) == "LW"] <- "W"
levels(test_y2)[levels(test_y2) == "CAM"| levels(test_y2) == "CM"] <- "CM"


train_y2 <- train_y
levels(train_y2)[levels(train_y2) == "RW"| levels(train_y2) == "LW"] <- "W"
levels(train_y2)[levels(train_y2) == "CAM"| levels(train_y2) == "CM"] <- "CM"

unique(test_y2)
```

```{r}
set.seed(123)
#plot pie chart again
cat<- table(factor(test_y2))
pie(cat, col = hcl.colors(length(cat), "BluYl"))
```

This is the new distribution of labels. Now we reproduce the same experiments, expecting a hefty increase in accuracy, with the price of ablation.

**3.7.1 Knn**

```{r}
set.seed(123)
prediction_knn2 <-knn(train = train, test = test, cl = train_y2, k = 51)
confusionMatrix(data=prediction_knn2, reference = test_y2)
```

```{r}
accuracy(table(test_y2, prediction_knn2))

```

**3.7.2 Logistic Regression**

```{r}
set.seed(123)
modellr2 <- nnet::multinom( train_y2~., data = train,trControl=crtl)
```

```{r}
set.seed(123)
prediction_lr2 <- predict(modellr2, test, type = "class")
accuracy(table(test_y2, prediction_lr2))
```

Now we create a confusion matrix

```{r}
set.seed(123)
confusionMatrix(data=prediction_lr2, reference = test_y2)
```

**3.7.3 Linear Discriminant Analysis**

```{r}
set.seed(123)
model_lda2 <- lda(train_y2~., data = train)
prediction_lda2 <-  predict(model_lda2, test, type = "class")
summary(model_lda2)
```

```{r}

set.seed(123)
accuracy(table(test_y2, prediction_lda2$class))
```

Now we plot the obtained model

```{r}

set.seed(123)
plot(model_lda2)
```

```{r}
set.seed(123)
confusionMatrix(data=prediction_lda2$class, reference = test_y2)
```

**3.7.4 Quadratic Discriminant Analysis**

```{r}
set.seed(123)
model_qda2 <- qda(train_y2~., data = train)
prediction_qda2 <-  predict(model_qda2, test, type = "class")
summary(model_qda2)
```

```{r}
set.seed(123)
accuracy(table(test_y2, prediction_qda2$class))

```

```{r}
set.seed(123)
confusionMatrix(data=prediction_qda2$class, reference = test_y2)
```

**3.8 Independent binary classification on confused labels**

**3.8.1 Dataset slicing and model preparation**

First we slice the rows with positions RW and LW and we prepare a train-test split for the mini dataset

```{r}
players_model_W <- players_model[players_model$player_positions == "RW" | players_model$player_positions == "LW",]
dim(players_model_W)
```

```{r}
set.seed(123)
## 70% of the sample size
train_ind_W <- sample(seq_len(nrow(players_model_W)), size = floor(0.7 * nrow(players_model_W)))

trainW <- players_model_W[train_ind_W, ]
testW <- players_model_W[-train_ind_W, ]

#We factorise the labels, so we can use them in our models.

train_yW <- as.factor(trainW[,25])
test_yW <- as.factor(testW[,25])
#remove labels from sets
trainW <- trainW[1:(length(trainW)-1)]
testW <- testW[1:(length(testW)-1)]
```

Now we do the same for the pair CM-CAM

```{r}
players_model_CM <- players_model[players_model$player_positions == "CM" | players_model$player_positions == "CAM",]
dim(players_model_CM)
```

```{r}
set.seed(123)
## 70% of the sample size
train_ind_CM <- sample(seq_len(nrow(players_model_CM)), size = floor(0.7 * nrow(players_model_CM)))

trainCM <- players_model_CM[train_ind_CM, ]
testCM <- players_model_CM[-train_ind_CM, ]

#We factorise the labels, so we can use them in our models.

train_yCM <- as.factor(trainCM[,25])
test_yCM <- as.factor(testCM[,25])
#remove labels from sets
trainCM <- trainCM[1:(length(trainCM)-1)]
testCM <- testCM[1:(length(testCM)-1)]
unique(test_yCM)
```

**3.8.2 Individual modelling** Now we try all models on the LW/RW pair

```{r}
set.seed(123)
modellrW <- nnet::multinom( train_yW~., data = trainW, trControl=crtl)

prediction_lrW <- predict(modellrW, testW, type = "class")
accuracy(table(test_yW, prediction_lrW))
```

```{r}
set.seed(123)
model_ldaW <- lda(train_yW~., data = trainW)
prediction_ldaW <-  predict(model_ldaW, testW, type = "class")

accuracy(table(test_yW, prediction_ldaW$class))

```

```{r}
set.seed(123)
model_qdaW <- qda(train_yW~., data = trainW)
prediction_qdaW <-  predict(model_qdaW, testW, type = "class")

accuracy(table(test_yW, prediction_qdaW$class))

```

```{r}
set.seed(123)
classW <- factor(c(train_yW))


accuracy_vectW <- c()
ksW<- c()

for(k1 in seq(1,50)) {
    test_predW <-knn(train = trainW, test = testW, cl = classW, k = k1)
    accuracy_vectW <- append(accuracy_vectW,accuracy(table(test_yW,test_predW)))
    ksW <- append(ksW, k1)
}

plot(ksW, accuracy_vectW, type = "p", col="blue", xlab="K's", ylab="accuracys", main="Accuracy vs K value plot")
```

```{r}
set.seed(123)
print('The best K in our case is:')
print(ksW[which.max(accuracy_vectW)])
print('And it gives us an accuracy of:' )
print(accuracy_vectW[which.max(accuracy_vectW)])
```

KNN seems to offer a little improvement in accuracy, with K=22. Now we do the same for the CM/CAM pair

```{r}
set.seed(123)
modellrCM <- nnet::multinom( train_yCM~., data = trainCM, trControl=crtl)

prediction_lrCM <- predict(modellrCM, testCM, type = "class")
accuracy(table(test_yCM, prediction_lrCM))
```

```{r}
set.seed(123)
model_ldaCM <- lda(train_yCM~., data = trainCM)
prediction_ldaCM <-  predict(model_ldaCM, testCM, type = "class")

accuracy(table(test_yCM, prediction_ldaCM$class))

```

```{r}
set.seed(123)
model_qdaCM <- qda(train_yCM~., data = trainCM)
prediction_qdaCM <-  predict(model_qdaCM, testCM, type = "class")

accuracy(table(test_yCM, prediction_qdaCM$class))

```

```{r}
set.seed(123)
classCM <- factor(c(train_yCM))


accuracy_vectCM <- c()
ksCM<- c()

for(k1 in seq(1,50)) {
    test_predCM <-knn(train = trainCM, test = testCM, cl = classCM, k = k1)
    accuracy_vectCM <- append(accuracy_vectCM,accuracy(table(test_yCM,test_predCM)))
    ksCM <- append(ksCM, k1)
}

plot(ksCM, accuracy_vectCM, type = "p", col="blue", xlab="K's", ylab="accuracys", main="Accuracy vs K value plot")
```

```{r}
set.seed(123)
print('The best K in our case is:')
print(ksCM[which.max(accuracy_vectCM)])
print('And it gives us an accuracy of:' )
print(accuracy_vectCM[which.max(accuracy_vectCM)])
```

In this case, Linear discriminant analysis sepparates the CM/CAM labels the best

Still, the accuracies are very small, almost 50%, so it is not worth including these models into the final ones.


##Regularization

#Ridge regression and Lasso

In this part we explore two shrinkage methods, namely Ridge regression and Lasso, also known as penalized regression methods. Through these techniques we can fit a model wher all the predictors are contained, but some coefficient estimates are shrinked towards 

First we take the players_market and remove the player_positions column that we will not consider for this task.

```{r}
players_market <-subset(players_m, select = -player_positions)
head(players_market)
```

```{r}
players_market$value_eur <- as.integer(players_market$value_eur)
# Left foot is -1 and Right foot is 1. Basically one-hot encoding but we only have 2 categories so its easy
players_market$preferred_foot[players_market[,"preferred_foot"]== "Left"] <- as.numeric(-1)
players_market$preferred_foot[players_market[,"preferred_foot"]== "Right"] <- as.numeric(1)
players_market$preferred_foot <- as.numeric(players_market$preferred_foot)
head(players_market)
```


```{r}
#norm
# normalization function
normalize <-function(x) { (x -min(x))/(max(x)-min(x))   }
colnames(players_market)
# normalize 
players_market_norm <- as.data.frame(lapply(players_market[, c(2:44)], normalize))
head(players_market_norm,5)
```



```{r}
attach(players_market_norm)
X <- model.matrix(value_eur ~.,players_market_norm)
y <- value_eur
```


#Ridge
Ridge uses  quadratic shrinking

```{r}
#ridge regression
grid.ridge <-10^seq(-4,2,length=100)
ridge.mod<-glmnet(X,y,alpha = 0,lambda = grid.ridge)
plot(ridge.mod, xvar="lambda", label= TRUE)
```
The plot illustrates how much the coefficients are penalized for different values of  lambda. Notice none of the coefficients are forced to be zero

Looking at the plot the features:

[2]age has a negative impact on th market value, which makes absolute sense since the older the player the less money he will be worth.
On the other hand, [4] international_reputation, [26] movement acceleration and [3] potential have a big positive impact on the market price

```{r}
colnames(players_market)
```



```{r}
set.seed(123)
# select n/2 observations for training set
train <- sample(1:nrow(X), nrow(X)/2)
test <- (-train)
y.test <- y[test]
```

```{r}
# fit ridge regression on the training set
ridge.mod <- glmnet(X[train, ], y[train], alpha = 0,
                    lambda = grid.ridge, thresh = 1e-12)
```

We estimate the test MSE for one lambda value, e.g lambda = 9
```{r}
#We estimate the test MSE for one lambda value, e.g lambda = 9
ridge.pred <- predict(ridge.mod, s = 9, newx = X[test, ], type="response")
mean((ridge.pred - y.test)^2)
```
let's see the coefficients for lambda = 9

```{r}
predict(ridge.mod, s = 9, exact = TRUE, type = "coefficients",
        x = X[train, ], y = y[train])[1:41, ]
```

We use cross-validation to choose the value of lambda

```{r}
set.seed(123)
cv.out.ridge <- cv.glmnet(X[train, ], y[train], alpha = 0, nfold=10)
cv.out.ridge$lambda[1:10]
```
```{r}
summary(cv.out.ridge$lambda)
```

```{r}
# The mean cross-validated error
cv.out.ridge$cvm[1:10]
```
The following plot shows the cross-validation curve (red dotted line) with upper and lower standard deviation curves along the lambda sequence (error bars). 

Two special values along the lambda sequence are indicated by the vertical dotted lines:

lambda.min is the value of lambda that gives minimum mean cross-validated error, while lambda.1se is the value of lambda that gives the most regularized model such that the cross-validated error is within one standard error of the minimum.

```{r}
plot(cv.out.ridge)
```

```{r}
# identify the best lambda value
i.bestlam <- which.min(cv.out.ridge$cvm)
i.bestlam 
```
```{r}
bestlam <- cv.out.ridge$lambda[i.bestlam]
bestlam
```
```{r}
# mean cross-validated error for best lambda
cv.out.ridge$cvm[i.bestlam]
```

```{r}
# estimate the test MSE 
ridge.pred <- predict(ridge.mod, s = bestlam,
                      newx = X[test, ])
#MSE
mean((ridge.pred - y.test)^2)
```

```{r}
# fit the coefficient with lambda=bestlam on all the data
out <- glmnet(X, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam)
```
```{r}
# residual sum of squares
ridge.rss <- sum((ridge.pred - y.test)^2)
ridge.rss
```

Let's compute the R squared. 
The R squared is a statistical measure of how well the regression
predictions approximate the real data points.
An R2 of 1 indicates that the regression predictions perfectly fit the data

```{r}
ridge.tss <- sum((y.test - mean(y.test)) ^ 2)  ## total sum of squares
ridge.rsq <- 1 - ridge.rss/ridge.tss  # R squared
ridge.rsq
```

#Lasso
Lasso uses absolute-value shrinking

```{r}
grid.lasso<-10^seq(-0.5,-8,length=100)
lasso.mod<-glmnet(X,y,alpha = 1,lambda = grid.lasso)
plot(lasso.mod, xvar="lambda", label = TRUE)
```

In the Lasso plot we can notice that some coefficients are forced to be zero. 
Morever, it is clear once again the impact of international_reputation, acceleration, potential and age on the estimation of a player market price.


We use cross-validation to choose the value of lambda

```{r}
set.seed(123)
cv.out.lasso <- cv.glmnet(X[train, ], y[train], alpha = 1, nfold=10)
cv.out.lasso$lambda[1:10]
```
```{r}
# apply lasso to the training set 
lasso.mod <- glmnet(X[train,], y[train], alpha=1, lambda=grid.lasso)
```

```{r}
# apply 10fold cross-validation to the training set
set.seed(123)
cv.out.lasso <- cv.glmnet(X[train,], y[train], alpha=1)
plot(cv.out.lasso)
```

```{r}
# estimate test MSE
bestlam <- cv.out.lasso$lambda.min
lasso.pred <- predict(lasso.mod, s=bestlam, newx=X[test,])
mean((lasso.pred-y.test)^2)
```

```{r}
# fit the model with best-lambda on all the data
lasso.coef <- predict(lasso.mod,type="coefficients",s=bestlam)[1:44,]
lasso.coef
```
We check which coefficients were forced to zero 
```{r}
lasso.coef[lasso.coef!=0]
length(lasso.coef[lasso.coef!=0])
which(lasso.coef==0)
```
Shooting, passing, dribbling, defending, physic, attacking_volleys, skill_ball_control and defending_sliding_tackle  were forced to be zero 
```{r}
# residual sum of squares
lasso.rss <- sum((lasso.pred - y.test)^2)
lasso.rss
```
```{r}
lasso.tss <- sum((y.test - mean(y.test)) ^ 2)  ## total sum of squares
lasso.rsq <- 1 - lasso.rss/lasso.tss  # R squared
lasso.rsq
```

```{r}
#ridge.rss vs lasso.rss
ridge.rss
lasso.rss
```

```{r}
#ridge.rsq vs lasso.rsq
ridge.rsq
lasso.rsq
```
Overall, we can say Lasso regression performs slightly better than Ridge Regression, given the higher R squared and lower Residual Sum of Squares. From both models is clear how age and agility are the features that mostly impact the market value (negatively and positively, respectively).
The results show that the models explain around 54% of the variance for the market value. 
R squared value between 0.5 and 0.7 is considered a moderate effect size.

```{r}
# compare with best subset selection using BIC and AIC
# apply best subset selection on the training data
library(leaps)
regfit.full <- regsubsets(value_eur~., data=players_market_norm[train,])
reg.summary <- summary(regfit.full)
reg.summary
```

```{r}
# select the model with best BIC
which.min(reg.summary$bic)
```

```{r}
coef(regfit.full, 8)
```
```{r}
mod.bic <- lm(value_eur~age + potential + pace+ international_reputation + movement_reactions  + movement_balance + power_stamina + power_strength, data = players_market_norm[train,])
```

```{r}
bic.pred <- predict(mod.bic, newdata=players_market_norm[test,])
mean((bic.pred-y.test)^2)
```

```{r}
# residual sum of squares
mode.bic.rss <- sum((bic.pred - y.test)^2)
mode.bic.rss
```

Let's compute the R squared. 

```{r}
mod.bic.tss <- sum((y.test - mean(y.test)) ^ 2)  ## total sum of squares
mod.bic.rsq <- 1 - mode.bic.rss/mod.bic.tss  # R squared
mod.bic.rsq
```
```{r}
# select the model with best AIC
which.min(reg.summary$cp)
```
```{r}
coef(regfit.full, 8)
```
The features selected are the same for AIC

```{r}
mod.aic <- lm(value_eur~age + potential + pace+ international_reputation + movement_reactions  + movement_balance + power_stamina + power_strength, data = players_market_norm[train,])
```

```{r}
aic.pred <- predict(mod.aic, newdata=players_market_norm[test,])
mean((aic.pred-y.test)^2)
```


**5. Conclusion and further research** 

All in all, position classification is possible for some distinct areas of the football field, but for some specific ones is quite impossible, in the case of multiclass classification. We have tried some specific models for RW&LW, and CM&CAM, respectively, but the results we obtained were not far from random. This is because multiple footballers have the necessary attributes to equally play in multiple spots. In order to improve classification, a multilabel approach on all the player positions would be better.

On one hand, football is a very heterogeneous sport and often the values of the attributes cannot explain as a whole the position of a player since his style of play heavily influence how the role is interpreted and consequently where exactly the player acts on the field. On the other hand, we would also like to believe that with sufficient data, even effective positioning of real players could be calculated.

As regard players' market price, an R squared of 0.54 is a decent value, but not remarkable. Of course age plays a big role, since young footballers have way larger margins of improvement; but it is also true that aspects like the contract expiry, which was not provided, have a significant effect in real life. When the date of expiry of a player approaches, the club is more inclined to agree even a lower price in order not to lose this athlete for free. The feature with the undisputed heavier positive impact is international_reputation and his come as no surprise. Football players that have a huge fan base (for example on social media) have a higher price since the club can profit from his visibility, that will result in more supporters and more revenue. 
Market value is the result of plenty of different information, some of which are very difficult to add among the features. For example, the current form of a player greatly influence his value: a striker scoring for the past 9 consecutive  matches will surely see his value skyrocket. With additional information like these, the models would definitely improve their performance. 
